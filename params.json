{"body":"# Say Hello to OpenPipeline\r\nOpenPipeline is new open source software for crawling, parsing, analyzing and routing documents. It ties together otherwise incomplete solutions for enterprise search and document processing. OpenPipeline provides a common architecture for connectors to data sources, file filters, text analyzers and modules to distribute documents across a network. It includes a job scheduler and a full UI with a point-and-click interface.\r\n\r\nIt comes fully functional with prebuilt components, but also integrates third-party modules. Plugins for crawling content management systems, parsing special file formats, and performing text analytics are available.\r\n\r\n## What’s New in 0.9?\r\n0.9 incorporates some architectural changes. We’ve done some housekeeping: logging and error reporting are better. We’ve made Connector an abstract class, and given Stage more access to its environment. There are many bug fixes. We’ve started on a new scheduler. And we added a WebCrawler connector. See CHANGELOG.txt in the download for the full list.\r\n\r\nAs for the WebCrawler, we elected to build a simple one. It has an architecture that, with a little tweaking, could support very large, web-scale crawls. It’s not quite there yet, and could stand a bit of refactoring, but it works. (Also — take a look at the plugins page — Findwise has a pretty good crawler.)\r\n\r\n## Roadmap\r\nSo where is OpenPipeline going? Here is our current thinking:\r\n\r\n### Some Short-Term Goals for OpenPipeline\r\n\r\n* Add tabs to the job configuration page so you can jump directly to the page you need.\r\n* Finish the Developer’s Guide\r\n* Create a contributor agreement so third parties can contribute to core OpenPipeline code.\r\n\r\n## Longer-Term Goals\r\n\r\n### Make it possible to install plugins automatically.\r\n\r\nThe idea is that you should be able to click a link in the admin interface that points to an external site, and have the system automatically download a plugin and install it. To get this to happen we’ll have to resolve a few issues: first, a plugin may require more than just a jar file. You may also need to download a config page, some supporting jars, and other resources. The download process will have to copy these things to the right places. It would be better if each plugin got its own subdir and everything it needed went there. This means we’d have to serve config pages from a location not in the webapps directory. The other issue is that the current way of loading plugins requires a server restart. This isn’t fatal.\r\n\r\n### Get OpenPipeline to run in a Hadoop-like manner.\r\n\r\nIdeally you would be able to create a pipeline, tell the system how many remote systems to run it on, and turn it loose. This requires quite a bit of coordination and control.","note":"Don't delete this file! It's used internally to help with page regeneration.","name":"Openpipeline","google":"","tagline":"OpenPipeline"}